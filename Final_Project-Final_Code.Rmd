---
title: "hyperparameter tuning h2o gbm"
author: "William Foote - 305134696"
date: "12/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(recipes)
library(rsample)
library(h2o)
```

# With h2o

# Initializing

```{r}
# I have 16 gb of RAM and was instructed to let h2o only use a max of a portion of that.
# h2o.init initializes the h2o interface with runs using Java

h2o.init(max_mem_size = "10G") 

# Recipe allows me to get the data in a cleaner format if necessary
blueprint <- recipe(Y ~ ., data = train) %>%
  step_other(all_nominal(), threshold = 0.005)


# Create training & test sets for h2o
train_h2o <- prep(blueprint, training = train, retain = TRUE) %>%
  juice() %>%
  as.h2o()
test_h2o <- prep(blueprint, training = train) %>%
  bake(new_data = test) %>%
  as.h2o()

# h2o needs the names of the variables in a specific format, which I do below
Y <- "Y"
X <- setdiff(names(train), c("Y", "Id"))
```

```{r}
# Construct a large Cartesian hyper-parameter space
ntrees_opts = c(10000)       # early stopping will stop earlier
max_depth_opts = seq(1,20)
min_rows_opts = c(1,5,10,20,50,100)
learn_rate_opts = seq(0.001,0.05, length.out = 10)
sample_rate_opts = seq(0.3,1,0.05)
col_sample_rate_opts = seq(0.3,1,0.05)
col_sample_rate_per_tree_opts = seq(0.3,1,0.05)

# Make this into a list
hyper_params = list(ntrees = ntrees_opts,
                    max_depth = max_depth_opts,
                    min_rows = min_rows_opts,
                    learn_rate = learn_rate_opts,
                    sample_rate = sample_rate_opts,
                    col_sample_rate = col_sample_rate_opts,
                    col_sample_rate_per_tree = col_sample_rate_per_tree_opts
)

# Search a random subset of these hyper-parmameters. Max runtime 
# and max models are enforced, and the search will stop after we 
# don't improve much over the best 5 random models.
search_criteria = list(strategy = "RandomDiscrete",
                       max_runtime_secs = 600,
                       max_models = 100,
                       stopping_metric = "AUTO",
                       stopping_tolerance = 0.00001,
                       stopping_rounds = 5,
                       seed = 123456)

gbm_grid <- h2o.grid("gbm",
                     grid_id = "mygrid",
                     x = X,
                     y = Y,
                     # faster to use a 80/20 split
                     # alternatively, use N-fold cross-validation:
                     training_frame = train_h2o,
                     nfolds = 10,
                     # Gaussian is best for MSE loss, but can try 
                     # other distributions ("laplace", "quantile"):
                     distribution="gaussian",
                     # stop as soon as mse doesn't improve by 
                     # more than 0.1% on the validation set, 
                     # for 2 consecutive scoring events:
                     stopping_rounds = 2,
                     stopping_tolerance = 1e-3,
                     stopping_metric = "MSE",
                     # how often to score (affects early stopping):
                     score_tree_interval = 100,
                     ## seed to control the sampling of the 
                     ## Cartesian hyper-parameter space:
                     seed = 123456,
                     hyper_params = hyper_params,
                     search_criteria = search_criteria)
gbm_sorted_grid <- h2o.getGrid(grid_id = "mygrid", sort_by = "rmse")
print(gbm_sorted_grid)
best_model <- h2o.getModel(gbm_sorted_grid@model_ids[[1]])
summary(best_model)

length(gbm_sorted_grid@model_ids)
as.data.frame(summary(gbm_sorted_grid))
```
# For submission

```{r}
# pred.boost.7 <- h2o.predict(best_model, test_h2o)
# sub.boost.7 <- cbind(test$Id, as.matrix(pred.boost.7))
# colnames(sub.boost.7) <- c("Id", "pred")
# write.csv(sub.boost.7, "sub_boost_final.csv", row.names = FALSE)
```

